{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d661f6b-f8e1-4c2b-82b8-4c7c15e9569c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"supabase_to_landing\").getOrCreate()\n",
    "\n",
    "# Configuração JDBC do Supabase\n",
    "jdbc_url = \"jdbc:postgresql://aws-1-sa-east-1.pooler.supabase.com:6543/postgres?sslmode=require\"\n",
    "\n",
    "db_properties = {\n",
    "    \"user\": \"postgres.flrkrtvkykwlydijicwk\",\n",
    "    \"password\": \"Projeto2025\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Lista das tabelas com nome do Supabase\n",
    "tabelas_supabase = [\n",
    "    '\"Tabela 1\"',\n",
    "    '\"Tabela 2\"',\n",
    "    '\"Tabela 3\"',\n",
    "    '\"Tabela 4\"',\n",
    "    '\"Tabela 5\"',\n",
    "    '\"Tabela 6\"',\n",
    "    '\"Tabela 7\"',\n",
    "    '\"Tabela 8\"',\n",
    "    '\"Tabela 9\"',\n",
    "    '\"Tabela 10\"'\n",
    "]\n",
    "\n",
    "# Caminho onde os CSV serão salvos DENTRO DO VOLUME\n",
    "base_path = \"/Volumes/workspace/landing/supabase/\"\n",
    "\n",
    "for tabela in tabelas_supabase:\n",
    "    nome_limpo = tabela.replace('\"', '').lower().replace(\" \", \"_\")\n",
    "\n",
    "    print(f\"\uD83D\uDCE5 Lendo tabela: {tabela}\")\n",
    "\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=tabela, properties=db_properties)\n",
    "\n",
    "    output_path = base_path + nome_limpo\n",
    "\n",
    "    df.write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "\n",
    "    print(f\"✅ Salvou CSV em: {output_path}\")\n",
    "\n",
    "print(\"\uD83C\uDF89 Processo finalizado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "781ddd46-e52f-4e21-b696-fa4911f14000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_check = spark.read.option(\"header\", True).csv(output_path)\n",
    "print(f\"➡️ Linhas salvas em CSV de {tabela}: {df_check.count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Conexao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}