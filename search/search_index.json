{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto de Engenharia de Dados","text":"<p>Este projeto demonstra um pipeline simples de engenharia de dados, incluindo:</p> <ul> <li>Gera\u00e7\u00e3o de dados fict\u00edcios com Faker</li> <li>Armazenamento de tabelas simuladas</li> <li>Efetuar leitura, padroniza\u00e7\u00e3o e tratamento de dados.</li> <li>Manipula\u00e7\u00e3o e an\u00e1lise usando notebooks</li> <li>Abordagem educacional para pr\u00e1tica de fundamentos</li> </ul> <p>Este projeto tem como finalidade desenvolver habilidades fundamentais em Engenharia de Dados, por meio da cria\u00e7\u00e3o de conjuntos de dados fict\u00edcios, do tratamento e an\u00e1lise dessas informa\u00e7\u00f5es e da documenta\u00e7\u00e3o estruturada do processo.</p> <p>A base do projeto envolve um conjunto de notebooks em Python e rotinas de gera\u00e7\u00e3o de dados simulados utilizando a biblioteca Faker, com posterior limpeza, consolida\u00e7\u00e3o e explora\u00e7\u00e3o anal\u00edtica dos dados referentes a um cen\u00e1rio fict\u00edcio de vendas de vinhos.</p>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias utilizadas","text":"Tecnologia Finalidade Python 3.x Processamento e gera\u00e7\u00e3o dos dados Faker Cria\u00e7\u00e3o de dados simulados realistas Pandas Tratamento, limpeza e organiza\u00e7\u00e3o Jupyter Notebook Execu\u00e7\u00e3o interativa das an\u00e1lises MkDocs Documenta\u00e7\u00e3o do projeto"},{"location":"#repositorio","title":"Reposit\u00f3rio","text":"<p>\ud83d\udc49 GitHub: ProjetoEngDados</p>"},{"location":"arquitetura/","title":"Arquitetura Geral","text":"<p>A arquitetura do projeto \u00e9 composta por:</p> <pre><code>[Python + Faker] \u2192 [Gera\u00e7\u00e3o de datasets]\n                     \u2193\n                  CSVs locais\n                     \u2193\n                 Jupyter Notebook\n                    An\u00e1lises\n</code></pre>"},{"location":"arquitetura/#componentes","title":"Componentes","text":"Componente Fun\u00e7\u00e3o <code>BibliotecaFaker.py</code> Gera dados sint\u00e9ticos com Faker <code>tabelas/</code> Guarda os CSVs gerados Notebooks Manipulam e analisam os dados"},{"location":"arquitetura/#objetivo","title":"Objetivo","text":"<p>Criar uma experi\u00eancia simples de ponta a ponta, refor\u00e7ando:</p> <ul> <li>Cria\u00e7\u00e3o de dados para testes</li> <li>Leitura e an\u00e1lise dos arquivos</li> <li>Aplica\u00e7\u00e3o de conceitos b\u00e1sicos de engenharia de dados</li> </ul>"},{"location":"conexao/","title":"Notebook \u2013 Conex\u00e3o","text":"<p>Conecta-se ao Supabase para persistir as tabelas e integra com o Databricks para armazenamento e processamento dos dados.</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"supabase_to_landing\").getOrCreate()\n\n# Configura\u00e7\u00e3o JDBC do Supabase\njdbc_url = \"jdbc:postgresql://aws-1-sa-east-1.pooler.supabase.com:6543/postgres?sslmode=require\"\n\ndb_properties = {\n    \"user\": \"postgres.flrkrtvkykwlydijicwk\",\n    \"password\": \"Projeto2025\",\n    \"driver\": \"org.postgresql.Driver\"\n}\n\n# Lista das tabelas com nome do Supabase\ntabelas_supabase = [\n    '\"Tabela 1\"',\n    '\"Tabela 2\"',\n    '\"Tabela 3\"',\n    '\"Tabela 4\"',\n    '\"Tabela 5\"',\n    '\"Tabela 6\"',\n    '\"Tabela 7\"',\n    '\"Tabela 8\"',\n    '\"Tabela 9\"',\n    '\"Tabela 10\"'\n]\n\n# Caminho onde os CSV ser\u00e3o salvos DENTRO DO VOLUME\nbase_path = \"/Volumes/workspace/landing/dados_vinhos/\"\n\nfor tabela in tabelas_supabase:\n    nome_limpo = tabela.replace('\"', '').lower().replace(\" \", \"_\")\n\n    print(f\"\ud83d\udce5 Lendo tabela: {tabela}\")\n\n    df = spark.read.jdbc(url=jdbc_url, table=tabela, properties=db_properties)\n\n    output_path = base_path + nome_limpo\n\n    df.write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n\n    print(f\"\u2705 Salvou CSV em: {output_path}\")\n\nprint(\"\ud83c\udf89 Processo finalizado!\")\n</code></pre> <pre><code>df_check = spark.read.option(\"header\", True).csv(output_path)\nprint(f\"\u27a1\ufe0f Linhas salvas em CSV de {tabela}: {df_check.count()}\")\n</code></pre>"},{"location":"database/","title":"Banco de Dados e Modelo ER","text":"<p>Esta p\u00e1gina descreve as tabelas, chaves e relacionamentos principais.</p> <p>Nota: se voc\u00ea tiver um diagrama ER (arquivo .png/.drawio), coloque-o em <code>docs/assets/</code> e referencie aqui.</p>"},{"location":"database/#tabelas-exemplo","title":"Tabelas (exemplo)","text":"<ul> <li>clientes</li> <li>id_cliente (PK)</li> <li>nome</li> <li>email</li> <li> <p>data_cadastro</p> </li> <li> <p>pedidos</p> </li> <li>id_pedido (PK)</li> <li>id_cliente (FK -&gt; clientes.id_cliente)</li> <li>data_pedido</li> <li> <p>valor_total</p> </li> <li> <p>itens_pedido</p> </li> <li>id_item (PK)</li> <li>id_pedido (FK -&gt; pedidos.id_pedido)</li> <li>produto</li> <li>quantidade</li> <li>preco_unitario</li> </ul>"},{"location":"database/#como-gerar-o-diagrama-er","title":"Como gerar o diagrama ER","text":"<ol> <li>Exportar esquema do banco (DDL) ou criar um arquivo <code>.drawio</code>.</li> <li>Salvar imagem em <code>docs/assets/er_diagrama.png</code>.</li> <li>Inserir imagem usando Markdown: `![ER Diagram](assets/er_diagrama.</li> </ol>"},{"location":"datasets/","title":"Dataset Fict\u00edcio \u2014 Vendas de Vinhos","text":"<p>O dataset deste projeto foi totalmente gerado de forma sint\u00e9tica utilizando a biblioteca Faker, simulando um cen\u00e1rio realista de vendas de vinhos, com informa\u00e7\u00f5es completas de cliente, produto e venda. Cada linha representa uma transa\u00e7\u00e3o individual.</p>"},{"location":"datasets/#estrutura-dos-dados","title":"Estrutura dos Dados","text":"<p>Campos presentes no dataset:</p> <ul> <li><code>id_venda</code></li> <li><code>data_venda</code></li> <li><code>cliente_id</code></li> <li><code>cliente_nome</code></li> <li><code>cliente_email</code></li> <li><code>cliente_cidade</code></li> <li><code>cliente_estado</code></li> <li><code>tipo_vinho</code></li> <li><code>rotulo</code></li> <li><code>uva</code></li> <li><code>pais_origem</code></li> <li><code>teor_alcoolico</code></li> <li><code>safra</code></li> <li><code>preco</code></li> <li><code>quantidade</code></li> <li><code>estabelecimento</code></li> <li><code>status_venda</code></li> </ul>"},{"location":"datasets/#geracao-dos-dados","title":"Gera\u00e7\u00e3o dos dados","text":"<p>\u2022 O script respons\u00e1vel \u00e9 o arquivo BibliotecaFaker.py. \u2022 Ele gera automaticamente 10 arquivos CSV. \u2022 Cada arquivo cont\u00e9m 20.000 registros. \u2022 O total do dataset completo \u00e9 de 200.000 linhas.</p>"},{"location":"datasets/#localizacao-dos-arquivos","title":"Localiza\u00e7\u00e3o dos arquivos:","text":"<p>\u2022 Todos os CSV s\u00e3o salvos na pasta tabelas. \u2022 Os nomes seguem o padr\u00e3o tabela_vendas_vinhos_1.csv at\u00e9 tabela_vendas_vinhos_10.csv.</p>"},{"location":"datasets/#utilizacao-no-projeto","title":"Utiliza\u00e7\u00e3o no projeto:","text":"<p>\u2022 Esses arquivos s\u00e3o lidos inicialmente no notebook Conexao.ipynb. \u2022 Em seguida passam por tratamento e padroniza\u00e7\u00e3o no Vinhos 001. \u2022 S\u00e3o analisados no Vinhos 002. \u2022 Depois seguem para as camadas Silver e Gold nos notebooks seguintes.</p>"},{"location":"faker/","title":"Gera\u00e7\u00e3o de Dados com Faker","text":"<p>O arquivo <code>BibliotecaFaker.py</code> \u00e9 respons\u00e1vel por criar dados sint\u00e9ticos para simular vendas de vinhos.</p>"},{"location":"faker/#principais-funcionalidades","title":"Principais funcionalidades","text":"<ul> <li>Uso da biblioteca Faker</li> <li>Cria\u00e7\u00e3o de m\u00faltiplas tabelas de vendas</li> <li> <p>Campos gerados:</p> </li> <li> <p>Nome do cliente  </p> </li> <li>Cidade  </li> <li>Quantidade  </li> <li>Tipo de vinho  </li> <li>Valor da venda  </li> <li>Data da compra  </li> </ul>"},{"location":"faker/#trecho-de-codigo","title":"Trecho de c\u00f3digo","text":"<pre><code>from faker import Faker\nfake = Faker()\n\ndef gerar_dado():\n    return {\n        \"cliente\": fake.name(),\n        \"cidade\": fake.city(),\n        \"vinho\": fake.color_name(),\n        \"quantidade\": fake.random_int(1, 12),\n        \"valor\": fake.pyfloat(positive=True),\n        \"data_venda\": fake.date_this_year()\n    }\n</code></pre>"},{"location":"faker/#saida","title":"Sa\u00edda","text":"<p>Os dados gerados s\u00e3o salvos em arquivos CSV dentro da pasta <code>tabelas/</code>.</p>"},{"location":"notebooks/","title":"Notebooks e Transforma\u00e7\u00f5es","text":"<p>Sequ\u00eancia de notebooks</p> <p>Os notebooks no Databricks seguem a ordem abaixo:</p> Ordem Notebook Descri\u00e7\u00e3o 0\ufe0f\u20e3 <code>Conexao</code> Cria conex\u00e3o com o Supabase 1\ufe0f\u20e3 <code>Vinhos 001</code> Cria\u00e7\u00e3o do banco 2\ufe0f\u20e3 <code>Vinhos 002</code> Bronze \u2192 Silver 3\ufe0f\u20e3 <code>Vinhos 003</code> Enriquecimento e joins 4\ufe0f\u20e3 <code>Vinhos 004</code> Delta Lake + SCD Type 2"},{"location":"notebooks/#exemplo-de-comando-delta","title":"Exemplo de comando Delta","text":"<pre><code>MERGE INTO silver_vinhos AS tgt \nUSING bronze_vinhos AS src\nON tgt.id = src.id\nWHEN MATCHED THEN UPDATE SET *\nWHEN NOT MATCHED THEN INSERT *\n</code></pre>"},{"location":"requisitos/","title":"Requisitos","text":""},{"location":"requisitos/#requisitos-de-software","title":"Requisitos de software","text":"<p>Para executar o projeto localmente, \u00e9 necess\u00e1rio ter instalado:</p> <ul> <li>Python 3.10 ou superior</li> <li>pip (gerenciador de pacotes do Python)</li> <li>Jupyter Notebook ou JupyterLab</li> <li>Git (para clonar o reposit\u00f3rio)</li> </ul>"},{"location":"requisitos/#bibliotecas-python-necessarias","title":"Bibliotecas python necess\u00e1rias","text":"<p>Instale as depend\u00eancias executando:</p> <pre><code>pip install pandas faker numpy matplotlib jupyter pyspark\n</code></pre>"},{"location":"requisitos/#requisitos-databricks","title":"Requisitos Databricks","text":"<p>Schemas:</p> <ul> <li>workspace.landing</li> <li>workspace.bronze_vinhos</li> <li>workspace.silver_vinhos</li> <li>workspace.gold_vinhos</li> </ul> <p>Volume:</p> <ul> <li>workspace.landing.dados_vinhos</li> </ul> <p>Runtime recomendado:</p> <ul> <li>Databricks Runtime 12.0+</li> </ul>"},{"location":"requisitos/#requisitos-para-ingestao-jdbc-supabase","title":"Requisitos para Ingest\u00e3o JDBC (Supabase)","text":"<ul> <li>Conta no Supabase</li> <li>Banco PostgreSQL</li> <li>Credenciais v\u00e1lidas</li> <li>SSL habilitado (sslmode=require)</li> </ul>"},{"location":"tabelas/","title":"Tabelas Geradas","text":"<p>As tabelas produzidas pelo script Faker s\u00e3o armazenadas em:</p> <pre><code>/tabelas/\n</code></pre> <p>Arquivos exportados:</p> <ul> <li><code>tabela_vendas_vinhos.csv</code></li> <li><code>tabela_vendas_vinhos_2.csv</code></li> <li><code>tabela_vendas_vinhos_3.csv</code></li> </ul> <p>Cada arquivo representa um lote de dados gerados.</p>"},{"location":"tabelas/#estrutura-dos-dados","title":"Estrutura dos dados","text":"Campo Descri\u00e7\u00e3o cliente Nome do comprador cidade Cidade de origem vinho Tipo de vinho quantidade Quantidade vendida valor Valor monet\u00e1rio da venda data_venda Data da transa\u00e7\u00e3o <p>Esses arquivos s\u00e3o consumidos nos notebooks para an\u00e1lise posterior.</p>"},{"location":"vinhos1/","title":"Notebook \u2013 Vinhos 001","text":"<pre><code>CREATE SCHEMA IF NOT EXISTS workspace.landing\nCOMMENT 'Schema/Database para dados bronze (delta)';\n\nCREATE VOLUME IF NOT EXISTS workspace.landing.dados_vinhos\nCOMMENT 'Volume para dados brutos criados no schema/database landing';\n\nCREATE SCHEMA IF NOT EXISTS workspace.bronze_vinhos\nCOMMENT 'Schema/Database para dados bronze (delta)';\n\nCREATE SCHEMA IF NOT EXISTS workspace.silver_vinhos\nCOMMENT 'Schema/Database para dados silver (delta)';\n\nCREATE SCHEMA IF NOT EXISTS workspace.gold_vinhos\nCOMMENT 'Schema/Database para dados gold (delta) - modelagem dimensional';\n</code></pre>"},{"location":"vinhos2/","title":"Notebook \u2013 Vinhos 002","text":"<p>Mostra todos os arquivos que est\u00e3o dentro do Volume chamado \"dados\" que foi criado dentro do cat\u00e1logo workspace, schema/database default.</p> <pre><code>display(dbutils.fs.ls('/Volumes/workspace/landing/dados_vinhos'))\n</code></pre> <p>Gera um dataframe para cada arquivo que est\u00e1 no Volume \"dados\".</p> <pre><code>caminho_landing = '/Volumes/workspace/landing/dados_vinhos'\n\ndf_tabela_vendas_vinhos_1   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_1/*.csv\")\ndf_tabela_vendas_vinhos_2   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_2/*.csv\")\ndf_tabela_vendas_vinhos_3   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_3/*.csv\")\ndf_tabela_vendas_vinhos_4   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_4/*.csv\")\ndf_tabela_vendas_vinhos_5   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_5/*.csv\")\ndf_tabela_vendas_vinhos_6   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_6/*.csv\")\ndf_tabela_vendas_vinhos_7   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_7/*.csv\")\ndf_tabela_vendas_vinhos_8   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_8/*.csv\")\ndf_tabela_vendas_vinhos_9   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_9/*.csv\")\ndf_tabela_vendas_vinhos_10   = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"{caminho_landing}/tabela_10/*.csv\")\n</code></pre> <p>Adiciona uma nova coluna (metadado) de data e hora de processamento e nome do arquivo de origem.</p> <pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_tabela_vendas_vinhos_1   = df_tabela_vendas_vinhos_1.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_1.csv\"))\ndf_tabela_vendas_vinhos_2   = df_tabela_vendas_vinhos_2.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_2.csv\"))\ndf_tabela_vendas_vinhos_3   = df_tabela_vendas_vinhos_3.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_3.csv\"))\ndf_tabela_vendas_vinhos_4   = df_tabela_vendas_vinhos_4.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_4.csv\"))\ndf_tabela_vendas_vinhos_5   = df_tabela_vendas_vinhos_5.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_5.csv\"))\ndf_tabela_vendas_vinhos_6   = df_tabela_vendas_vinhos_6.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_6.csv\"))\ndf_tabela_vendas_vinhos_7   = df_tabela_vendas_vinhos_7.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_7.csv\"))\ndf_tabela_vendas_vinhos_8   = df_tabela_vendas_vinhos_8.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_8.csv\"))\ndf_tabela_vendas_vinhos_9   = df_tabela_vendas_vinhos_9.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_9.csv\"))\ndf_tabela_vendas_vinhos_10   = df_tabela_vendas_vinhos_10.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"tabela_vendas_vinhos_10.csv\"))\n</code></pre> <p>Salva os dataframes em arquivos delta lake (formato de arquivo) no schema/database \"bronze\". As tabelas geradas s\u00e3o do tipo MANAGED (gerenciadas).</p> <pre><code>df_tabela_vendas_vinhos_1.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_1\")\ndf_tabela_vendas_vinhos_2.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_2\")\ndf_tabela_vendas_vinhos_3.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_3\")\ndf_tabela_vendas_vinhos_4.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_4\")\ndf_tabela_vendas_vinhos_5.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_5\")\ndf_tabela_vendas_vinhos_6.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_6\")\ndf_tabela_vendas_vinhos_7.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_7\")\ndf_tabela_vendas_vinhos_8.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_8\")\ndf_tabela_vendas_vinhos_9.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_9\")\ndf_tabela_vendas_vinhos_10.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze_vinhos.tabela_vendas_vinhos_10\")\n</code></pre> <p>Verifica os dados gravados no formato delta lake tipo MANAGED na camada bronze.</p> <pre><code>SHOW TABLES IN bronze_vinhos\n</code></pre> <p>Mostra os detalhes de uma tabela delta lake.</p> <pre><code>DESCRIBE DETAIL bronze_vinhos.tabela_vendas_vinhos_1;\n</code></pre> <p>Mostra se a tabela \u00e9 MANAGED Ou EXTERNAL.</p> <pre><code>DESCRIBE EXTENDED bronze_vinhos.tabela_vendas_vinhos_1;\n</code></pre>"},{"location":"vinhos3/","title":"Notebook \u2013 Vinhos 003","text":"<p>Gera um dataframe para cada tabela delta de bronze.</p> <pre><code>df_tabela_vendas_vinhos_1   = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_1\")\ndf_tabela_vendas_vinhos_2     = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_2\")\ndf_tabela_vendas_vinhos_3   = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_3\")\ndf_tabela_vendas_vinhos_4  = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_4\")\ndf_tabela_vendas_vinhos_5    = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_5\")\ndf_tabela_vendas_vinhos_6     = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_6\")\ndf_tabela_vendas_vinhos_7    = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_7\")\ndf_tabela_vendas_vinhos_8 = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_8\")\ndf_tabela_vendas_vinhos_9    = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_9\")\ndf_tabela_vendas_vinhos_10  = spark.read.format(\"delta\").table(\"bronze_vinhos.tabela_vendas_vinhos_10\")\n</code></pre> <p>Adiciona uma nova coluna (metadado) de data e hora de processamento e nome do arquivo de origem.</p> <pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_tabela_vendas_vinhos_1   = df_tabela_vendas_vinhos_1.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_1\"))\ndf_tabela_vendas_vinhos_2     = df_tabela_vendas_vinhos_2.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_2\"))\ndf_tabela_vendas_vinhos_3   = df_tabela_vendas_vinhos_3.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_3\"))\ndf_tabela_vendas_vinhos_4  = df_tabela_vendas_vinhos_4.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_4\"))\ndf_tabela_vendas_vinhos_5    = df_tabela_vendas_vinhos_5.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_5\"))\ndf_tabela_vendas_vinhos_6     = df_tabela_vendas_vinhos_6.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_6\"))\ndf_tabela_vendas_vinhos_7    = df_tabela_vendas_vinhos_7.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_7\"))\ndf_tabela_vendas_vinhos_8 = df_tabela_vendas_vinhos_8.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_8\"))\ndf_tabela_vendas_vinhos_9    = df_tabela_vendas_vinhos_9.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_9\"))\ndf_tabela_vendas_vinhos_10  = df_tabela_vendas_vinhos_10.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_tabela\", lit(\"tabela_vendas_vinhos_10\"))\n</code></pre> <p>Salva os dataframes em arquivos delta lake (formato de arquivo) no schema/database \"bronze\". As tabelas geradas s\u00e3o do tipo MANAGED (gerenciadas). Feito no Vinhos 002(anterior).</p> <pre><code>df_tabela_vendas_vinhos_1.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_1\")\ndf_tabela_vendas_vinhos_2.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_2\")\ndf_tabela_vendas_vinhos_3.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_3\")\ndf_tabela_vendas_vinhos_4.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_4\")\ndf_tabela_vendas_vinhos_5.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_5\")\ndf_tabela_vendas_vinhos_6.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_6\")\ndf_tabela_vendas_vinhos_7.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_7\")\ndf_tabela_vendas_vinhos_8.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_8\")\ndf_tabela_vendas_vinhos_9.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_9\")\ndf_tabela_vendas_vinhos_10.write.format('delta').mode(\"overwrite\").saveAsTable(\"bronze.tabela_vendas_vinhos_10\")\n</code></pre> <p>Maiusculas, tirando siglas, etc e gravando no formato delta no Silver; Aplicando Data Quality;</p> <pre><code>from pyspark.sql import functions as F\n\n# ---------- Helpers ----------\ndef _apply_name_rules(colname: str) -&gt; str:\n    \"\"\"Regras de renome: upper + prefixos 'CD_', 'VL_', etc.\"\"\"\n    n = colname.upper()\n    n = n.replace(\"CD_\", \"CODIGO_\")\n    n = n.replace(\"VL_\", \"VALOR_\")\n    n = n.replace(\"DT_\", \"DATA_\")\n    n = n.replace(\"NM_\", \"NOME_\")\n    n = n.replace(\"DS_\", \"DESCRICAO_\")\n    n = n.replace(\"NR_\", \"NUMERO_\")\n    n = n.replace(\"_UF\", \"_UNIDADE_FEDERATIVA\")\n    return n\n\ndef _safe_drop(df, cols):\n    \"\"\"Dropa colunas somente se existirem.\"\"\"\n    existing = set(df.columns)\n    to_drop = [c for c in cols if c in existing]\n    return df.drop(*to_drop) if to_drop else df\n\n# ---------- Core ----------\ndef renomear_colunas_managed(src_fqn: str, dest_fqn: str = None):\n    \"\"\"\n    L\u00ea uma managed table (Delta) do metastore, aplica regras de renome,\n    ajusta colunas de auditoria e salva como **managed table** via saveAsTable.\n    - src_fqn: 'schema.tabela' de origem (ex.: 'bronze.tabela_vendas_vinhos_1')\n    - dest_fqn: 'schema.tabela' de destino; se None, sobrescreve a pr\u00f3pria origem\n    \"\"\"\n    dest_fqn = dest_fqn or src_fqn\n\n    # L\u00ea como TABELA (managed)\n    df = spark.read.format(\"delta\").table(src_fqn)\n\n    # Renomeia todas as colunas de uma vez (evita conflito de rename em loop)\n    new_cols = [_apply_name_rules(c) for c in df.columns]\n    df = df.toDF(*new_cols)\n\n    # Remove colunas antigas, se existirem\n    df = _safe_drop(df, [\"DATA_HORA_BRONZE\", \"NOME_ARQUIVO\"])\n\n    # Adiciona colunas de auditoria pedidas\n    df = (df\n          .withColumn(\"NOME_ARQUIVO_BRONZE\", F.lit(src_fqn))     # origem rastre\u00e1vel\n          .withColumn(\"DATA_ARQUIVO_SILVER\", F.current_timestamp())\n         )\n\n    # Salva como **Managed Table** (sem LOCATION) \u2014 sobrescrevendo destino\n    (df.write\n       .format(\"delta\")\n       .mode(\"overwrite\")\n       .saveAsTable(dest_fqn))\n\n    return dest_fqn\n</code></pre> <pre><code>renomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_1\",   \"silver_vinhos.tabela_vendas_vinhos_1\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_2\",     \"silver_vinhos.tabela_vendas_vinhos_2\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_3\",   \"silver_vinhos.tabela_vendas_vinhos_3\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_4\",  \"silver_vinhos.tabela_vendas_vinhos_4\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_5\",    \"silver_vinhos.tabela_vendas_vinhos_5\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_6\",     \"silver_vinhos.tabela_vendas_vinhos_6\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_7\",    \"silver_vinhos.tabela_vendas_vinhos_7\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_8\",  \"silver_vinhos.tabela_vendas_vinhos_8\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_9\",    \"silver_vinhos.tabela_vendas_vinhos_9\")\nrenomear_colunas_managed(\"bronze_vinhos.tabela_vendas_vinhos_10\",  \"silver_vinhos.tabela_vendas_vinhos_10\")\n</code></pre> <p>Verifica os dados gravados no formato delta lake tipo MANAGED na camada bronze.</p> <pre><code>SHOW TABLES IN silver_vinhos\n</code></pre> <p>Mostra os detalhes de uma tabela delta lake.</p> <pre><code>DESCRIBE DETAIL silver_vinhos.tabela_vendas_vinhos_1;\n</code></pre> <p>Mostra se a tabela \u00e9 MANAGED Ou EXTERNAL.</p> <pre><code>DESCRIBE EXTENDED silver_vinhos.tabela_vendas_vinhos_1;\n--DESCRIBE TABLE EXTENDED tabela_vendas_vinhos_1_bronze_vinhos;\n</code></pre>"},{"location":"vinhos4/","title":"Notebook \u2013 Vinhos 004","text":"<p>Gerando um dataframe dos delta lake no container bronze do Azure Data Lake Storage.</p> <pre><code>df_tabela_vendas_vinhos_1   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_1\")\ndf_tabela_vendas_vinhos_2   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_2\")\ndf_tabela_vendas_vinhos_3   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_3\")\ndf_tabela_vendas_vinhos_4   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_4\")\ndf_tabela_vendas_vinhos_5   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_5\")\ndf_tabela_vendas_vinhos_6   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_6\")\ndf_tabela_vendas_vinhos_7   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_7\")\ndf_tabela_vendas_vinhos_8   = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_8\")\ndf_tabela_vendas_vinhos_9    = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_9\")\ndf_tabela_vendas_vinhos_10  = spark.read.format(\"delta\").table(\"silver_vinhos.tabela_vendas_vinhos_10\")\n</code></pre> <p>Unindo as tabelas usando unionByName.</p> <pre><code>df_tabela_combinada = df_tabela_vendas_vinhos_1 \\\n    .unionByName(df_tabela_vendas_vinhos_2) \\\n    .unionByName(df_tabela_vendas_vinhos_3) \\\n    .unionByName(df_tabela_vendas_vinhos_4) \\\n    .unionByName(df_tabela_vendas_vinhos_5) \\\n    .unionByName(df_tabela_vendas_vinhos_6) \\\n    .unionByName(df_tabela_vendas_vinhos_7) \\\n    .unionByName(df_tabela_vendas_vinhos_8) \\\n    .unionByName(df_tabela_vendas_vinhos_9) \\\n    .unionByName(df_tabela_vendas_vinhos_10)\n</code></pre> <p>Salvando a tabela combinada em uma nova tabela Delta, se necess\u00e1rio.</p> <pre><code>df_tabela_combinada.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_vinhos.tabela_vendas_vinhos_combinada\")\n</code></pre> <p>Adicionando metadados de data e hora de processamento e nome do arquivo de origem.</p> <p><pre><code>SELECT * FROM silver_vinhos.tabela_vendas_vinhos_combinada\n</code></pre> <pre><code>SELECT COUNT(*) FROM silver_vinhos.tabela_vendas_vinhos_combinada; \n</code></pre></p> <pre><code>DROP TABLE IF EXISTS gold_vinhos.dim_vendas_diarias;\n</code></pre> <pre><code>df_tabela_vendas_vinhos_1.createOrReplaceTempView(\"tabela_vendas_vinhos_combinada\")\n</code></pre> <pre><code>CREATE TABLE gold_vinhos.dim_vendas_diarias\nUSING DELTA\nAS\nSELECT \n    YEAR(DATA_VENDA) AS ANO,                        \n    MONTH(DATA_VENDA) AS MES,                       \n    DAY(DATA_VENDA) AS DIA,                         \n    STATUS_VENDA,                                  \n    SUM(QUANTIDADE) AS QUANTIDADE_VENDIDA,        \n    ROUND(SUM(CAST(REPLACE(PRECO, ',', '') AS DOUBLE)), 4) AS VALOR_VENDIDO \nFROM silver_vinhos.tabela_vendas_vinhos_combinada  \nWHERE STATUS_VENDA IS NOT NULL  \nGROUP BY \n    YEAR(DATA_VENDA),   \n    MONTH(DATA_VENDA),  \n    DAY(DATA_VENDA),    \n    STATUS_VENDA     \n</code></pre> <pre><code>DESCRIBE TABLE EXTENDED gold_vinhos.dim_vendas_diarias\n</code></pre> <pre><code>WITH vendas_diarias_relacional AS (\n    SELECT \n        YEAR(DATA_VENDA) AS ANO,                        \n        MONTH(DATA_VENDA) AS MES,                       \n        DAY(DATA_VENDA) AS DIA,                       \n        STATUS_VENDA,                                 \n        SUM(QUANTIDADE) AS QUANTIDADE_VENDIDA,          \n        ROUND(SUM(CAST(REPLACE(PRECO, ',', '') AS DOUBLE)), 4) AS VALOR_VENDIDO \n    FROM silver_vinhos.tabela_vendas_vinhos_combinada  \n    WHERE STATUS_VENDA IS NOT NULL  \n    GROUP BY \n        YEAR(DATA_VENDA),    \n        MONTH(DATA_VENDA),   \n        DAY(DATA_VENDA),     \n        STATUS_VENDA         \n)\nMERGE INTO gold_vinhos.dim_vendas_diarias AS dsv\nUSING vendas_diarias_relacional AS rsv\nON dsv.ANO = rsv.ANO AND dsv.MES = rsv.MES AND dsv.DIA = rsv.DIA AND dsv.STATUS_VENDA = rsv.STATUS_VENDA\n\nWHEN MATCHED AND (\n    dsv.QUANTIDADE_VENDIDA &lt;&gt; rsv.QUANTIDADE_VENDIDA OR\n    dsv.VALOR_VENDIDO &lt;&gt; ROUND(rsv.VALOR_VENDIDO, 4) \n) THEN\n    UPDATE SET \n        dsv.QUANTIDADE_VENDIDA = rsv.QUANTIDADE_VENDIDA,\n        dsv.VALOR_VENDIDO = ROUND(rsv.VALOR_VENDIDO, 4)  \n\nWHEN NOT MATCHED THEN\n    INSERT (\n        ANO,\n        MES,\n        DIA,\n        QUANTIDADE_VENDIDA,\n        VALOR_VENDIDO,\n        STATUS_VENDA\n    )\n    VALUES (\n        rsv.ANO,\n        rsv.MES,\n        rsv.DIA,\n        rsv.QUANTIDADE_VENDIDA,\n        ROUND(rsv.VALOR_VENDIDO, 4), \n        rsv.STATUS_VENDA\n    );\n</code></pre> <pre><code>SELECT * FROM gold_vinhos.dim_vendas_diarias\n</code></pre> <pre><code>DROP TABLE IF EXISTS gold_vinhos.dim_cliente\n</code></pre> <pre><code>CREATE TABLE gold_vinhos.dim_cliente\nUSING DELTA\nAS\nSELECT DISTINCT\n    CLIENTE_ID,              \n    CLIENTE_NOME,            \n    CLIENTE_EMAIL            \nFROM silver_vinhos.tabela_vendas_vinhos_combinada;\n</code></pre> <pre><code>DESCRIBE TABLE EXTENDED gold_vinhos.dim_cliente\n</code></pre> <pre><code>WITH localidade_relacional AS (\n    SELECT DISTINCT\n           CLIENTE_CIDADE,\n           CLIENTE_ESTADO,\n           PAIS_ORIGEM\n      FROM silver_vinhos.tabela_vendas_vinhos_combinada  \n)\nMERGE INTO\n    gold_vinhos.dim_localidade AS dl  \nUSING\n    localidade_relacional AS rl \nON rl.CLIENTE_CIDADE = dl.CIDADE AND rl.CLIENTE_ESTADO = dl.ESTADO AND rl.PAIS_ORIGEM = dl.PAIS\n\nWHEN MATCHED AND (\n    rl.CLIENTE_CIDADE &lt;&gt; dl.CIDADE OR\n    rl.CLIENTE_ESTADO &lt;&gt; dl.ESTADO OR\n    rl.PAIS_ORIGEM &lt;&gt; dl.PAIS\n) THEN\n    UPDATE SET\n        dl.CIDADE = rl.CLIENTE_CIDADE,\n        dl.ESTADO = rl.CLIENTE_ESTADO,\n        dl.PAIS = rl.PAIS_ORIGEM\n\nWHEN NOT MATCHED THEN\n    INSERT (CIDADE, ESTADO, PAIS)\n    VALUES (rl.CLIENTE_CIDADE, rl.CLIENTE_ESTADO, rl.PAIS_ORIGEM);\n</code></pre> <pre><code>select * from gold_vinhos.dim_localidade\n</code></pre> <pre><code>drop table if exists gold_vinhos.dim_dados_vinhos\n</code></pre> <pre><code>CREATE TABLE gold_vinhos.dim_dados_vinhos (\n    SK_VINHO             BIGINT GENERATED BY DEFAULT AS IDENTITY,\n    ID_VENDA             STRING,\n    TIPO_VINHO           STRING,\n    ROTULO               STRING,\n    UVA                  STRING,\n    TEOR_ALCOOLICO       STRING,\n    PRECO                STRING\n)\nUSING DELTA;\n</code></pre> <pre><code>DESCRIBE TABLE EXTENDED gold_vinhos.dim_dados_vinhos\n</code></pre> <pre><code>WITH dados_vinhos_relacional AS (\n    SELECT DISTINCT\n           ID_VENDA,            \n           PAIS_ORIGEM,\n           TIPO_VINHO,\n           ROTULO,\n           UVA,\n           TEOR_ALCOOLICO,\n           PRECO      \n      FROM silver_vinhos.tabela_vendas_vinhos_combinada \n)\nMERGE INTO\n    gold_vinhos.dim_dados_vinhos AS ddv\nUSING\n    dados_vinhos_relacional AS rdv\nON ddv.ID_VENDA = rdv.ID_VENDA    \n\nWHEN MATCHED AND (\n    ddv.TIPO_VINHO &lt;&gt; rdv.TIPO_VINHO OR \n    ddv.ROTULO &lt;&gt; rdv.ROTULO OR\n    ddv.UVA &lt;&gt; rdv.UVA OR\n    ddv.TEOR_ALCOOLICO &lt;&gt; rdv.TEOR_ALCOOLICO OR\n    ddv.PRECO &lt;&gt; rdv.PRECO\n) THEN\n    UPDATE SET \n        ddv.TIPO_VINHO = rdv.TIPO_VINHO,\n        ddv.ROTULO = rdv.ROTULO,\n        ddv.UVA = rdv.UVA,\n        ddv.TEOR_ALCOOLICO = rdv.TEOR_ALCOOLICO,\n        ddv.PRECO = rdv.PRECO\n\nWHEN NOT MATCHED THEN\n    INSERT (\n        ID_VENDA, TIPO_VINHO, ROTULO, UVA, TEOR_ALCOOLICO, PRECO\n    )\n    VALUES (\n        rdv.ID_VENDA, rdv.TIPO_VINHO, rdv.ROTULO, rdv.UVA, rdv.TEOR_ALCOOLICO, rdv.PRECO\n    );\n</code></pre> <pre><code>SELECT * FROM gold_vinhos.dim_dados_vinhos\n</code></pre> <pre><code>DROP TABLE IF EXISTS gold_vinhos.fato_vendas_concluidas\n</code></pre> <pre><code>CREATE TABLE gold_vinhos.fato_vendas_concluidas (\n    ANO INT,                    \n    MES INT,                   \n    DIA INT,                    \n    CIDADE STRING,              \n    ESTADO STRING,              \n    PAIS STRING,                \n    TIPO_VINHO STRING,          \n    CLIENTE_NOME STRING,        \n    CLIENTE_EMAIL STRING,       \n    CLIENTE_ID STRING,             \n    QUANTIDADE_VENDIDA DOUBLE,     \n    VALOR_VENDIDO DOUBLE \n\n)\nUSING DELTA;\n</code></pre> <pre><code>DESCRIBE TABLE EXTENDED gold_vinhos.fato_vendas_concluidas\n</code></pre> <pre><code>%sql\nWITH vendas_concluidas_relacional AS (\n    SELECT \n        YEAR(v.DATA_VENDA) AS ANO,               \n        MONTH(v.DATA_VENDA) AS MES,              \n        DAY(v.DATA_VENDA) AS DIA,               \n        l.CIDADE AS CIDADE,                \n        l.ESTADO AS ESTADO,                \n        l.PAIS AS PAIS,                         \n        dv.TIPO_VINHO AS TIPO_VINHO,            \n        c.CLIENTE_NOME AS CLIENTE_NOME,        \n        c.CLIENTE_EMAIL AS CLIENTE_EMAIL,      \n        c.CLIENTE_ID AS CLIENTE_ID,             \n        SUM(v.QUANTIDADE) AS QUANTIDADE_VENDIDA, \n        ROUND(SUM(CAST(REPLACE(v.PRECO, ',', '') AS DOUBLE)), 4) AS VALOR_VENDIDO \n    FROM silver_vinhos.tabela_vendas_vinhos_combinada AS v\n    JOIN gold_vinhos.dim_localidade AS l\n        ON v.CLIENTE_CIDADE = l.CIDADE \n        AND v.CLIENTE_ESTADO = l.ESTADO \n        AND v.PAIS_ORIGEM = l.PAIS\n    JOIN gold_vinhos.dim_dados_vinhos AS dv\n        ON v.TIPO_VINHO = dv.TIPO_VINHO\n    JOIN gold_vinhos.dim_cliente AS c\n        ON v.CLIENTE_ID = c.CLIENTE_ID\n    WHERE v.STATUS_VENDA = 'conclu\u00edda'  \n    GROUP BY \n        YEAR(v.DATA_VENDA), \n        MONTH(v.DATA_VENDA), \n        DAY(v.DATA_VENDA), \n        l.CIDADE, \n        l.ESTADO, \n        l.PAIS, \n        dv.TIPO_VINHO, \n        c.CLIENTE_NOME, \n        c.CLIENTE_EMAIL, \n        c.CLIENTE_ID\n)\n\nMERGE INTO gold_vinhos.fato_vendas_concluidas AS fvc\nUSING vendas_concluidas_relacional AS rsv\nON fvc.ANO = rsv.ANO \n   AND fvc.MES = rsv.MES\n   AND fvc.DIA = rsv.DIA\n   AND fvc.TIPO_VINHO = rsv.TIPO_VINHO\n   AND fvc.CIDADE = rsv.CIDADE\n   AND fvc.ESTADO = rsv.ESTADO\n   AND fvc.PAIS = rsv.PAIS\n   AND fvc.CLIENTE_ID = rsv.CLIENTE_ID\n   AND fvc.CLIENTE_NOME = rsv.CLIENTE_NOME\n   AND fvc.CLIENTE_EMAIL = rsv.CLIENTE_EMAIL\n\nWHEN MATCHED AND (\n    fvc.QUANTIDADE_VENDIDA &lt;&gt; rsv.QUANTIDADE_VENDIDA OR\n    fvc.VALOR_VENDIDO &lt;&gt; rsv.VALOR_VENDIDO\n) THEN\n    UPDATE SET \n        fvc.QUANTIDADE_VENDIDA = rsv.QUANTIDADE_VENDIDA,\n        fvc.VALOR_VENDIDO = rsv.VALOR_VENDIDO\n\n-- Quando n\u00e3o houver correspond\u00eancia (inserir novos dados)\nWHEN NOT MATCHED THEN\n    INSERT (\n        ANO,\n        MES,\n        DIA,\n        TIPO_VINHO,\n        CIDADE,\n        ESTADO,\n        PAIS,\n        CLIENTE_ID,\n        CLIENTE_NOME,\n        CLIENTE_EMAIL,\n        QUANTIDADE_VENDIDA,\n        VALOR_VENDIDO\n    )\n    VALUES (\n        rsv.ANO,\n        rsv.MES,\n        rsv.DIA,\n        rsv.TIPO_VINHO,\n        rsv.CIDADE,\n        rsv.ESTADO,\n        rsv.PAIS,\n        rsv.CLIENTE_ID,\n        rsv.CLIENTE_NOME,\n        rsv.CLIENTE_EMAIL,\n        rsv.QUANTIDADE_VENDIDA,\n        rsv.VALOR_VENDIDO\n    )\n</code></pre> <pre><code>SELECT * FROM gold_vinhos.fato_vendas_concluidas\n</code></pre>"}]}